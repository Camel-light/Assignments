{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Camel-light/Assignments/blob/main/assignement_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "**What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?**\n",
        "\n",
        "The torch.cat() function is used to concatenate the tensors of the images containing the number 3 and the images containing the number 7 into a single tensor.\n",
        "\n",
        "The view() function is then used to reshape this concatenated tensor into a 2D tensor, where each row represents a single image, and the number of columns equals the total number of pixels in each image.\n",
        "\n",
        "\n",
        "**Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?**\n",
        "Input (784 x 1) => weights (784 x 1) + bias => Linear1 =>  Output (1 x 1)\n",
        "\n",
        "\n",
        "**Why is it not possible to use the accuracy as loss function?**\n",
        "\n",
        "Because the accuracy changes only when the prediction changes from a 3 to a 7, and if we do only very small changes to the x, the prediction is likely not to change at all. The gradient is almost 0 everywhere.\n",
        "\n",
        "**What is the defined `mnist_loss` function doing?**\n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "The function calculates the loss between the predicted output and the target output. It takes two arguments: predictions and targets.\n",
        "\n",
        "The predictions argument represents the output of the neural network for a given input. This output is compared to the targets argument.\n",
        "\n",
        "The torch.where function is used to calculate the loss. It compares the targets tensor to the value 1 element-wise. If an element in the targets tensor is 1, the corresponding element in the predictions tensor is subtracted from 1. If an element in the targets tensor is 0, the corresponding element in the predictions tensor is returned as is. This creates a tensor of loss values.\n",
        "\n",
        "Finally, the mean method is called on the tensor of loss values, which calculates the average loss across all elements in the tensor. This average loss is returned as the output of the mnist_loss function.\n",
        "\n",
        "**Why do we need additionaly the sigmoid() function? What is it technically in our TLU?**\n",
        "\n",
        "The Sigmoid function is needed to ensure that the values given to the mnist_loss function are always between 0 and 1. \n",
        "In technical terms, it is the non-linear function of a TLU which allows to approximate non linear functions.\n",
        "\n",
        "**Again, what are mini batches, why are we using them and why should they be shuffeld?**\n",
        "\n",
        "Mini-batches are subsets of the training data that are used to update the parameters of the model during training. Instead of using the entire dataset to perform a single update, the dataset is divided into smaller, equally-sized batches. These batches are processed, and the parameters of the model are updated based on the average loss over the mini-batch.\n",
        "\n",
        "Using mini-batches has several advantages over using the entire dataset. First, it is computationally more efficient on GPUs because we can use parallelization to process multiple batches at the same time. Second, mini-batches are a compromise between measuring only one instance of loss function and every instance for each pair of output and predictions.Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model\n",
        "quickly and accurately.\n",
        "\n",
        "It is important to shuffle the mini-batches during training because it ensures that each mini-batch is representative of the entire dataset. If the data is not shuffled, the model may see similar examples in each mini-batch, which can lead to poor generalization and overfitting. Shuffling the mini-batches ensures that the model sees a diverse range of examples in each iteration, which can help the model to generalize better."
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxwyNuzj3DYu"
      },
      "outputs": [],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer.\n",
        "\n",
        "# Load data\n",
        "from fastai.vision.all import *\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n",
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "\n",
        "# Prepare data for training and validation\n",
        "train_x = torch.cat([stacked_threes[:500], stacked_sevens[:500]]).view(-1, 28*28)\n",
        "train_y = tensor([1]*500 + [0]*500).unsqueeze(1)\n",
        "valid_x = torch.cat([stacked_threes[500:], stacked_sevens[500:]]).view(-1, 28*28)\n",
        "valid_y = tensor([1]*(len(threes)-500) + [0]*(len(sevens)-500)).unsqueeze(1)\n",
        "train_dset = list(zip(train_x,train_y))\n",
        "valid_dset = list(zip(valid_x,valid_y))\n",
        "\n",
        "# Initialize weights and bias\n",
        "def init_params(size, std=1.0):\n",
        "    return (torch.randn(size)*std).requires_grad_()\n",
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)\n",
        "\n",
        "# Define model\n",
        "def linear1(xb):\n",
        "    return xb@weights + bias\n",
        "\n",
        "# Define loss function\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "# Calculate accuracy\n",
        "def accuracy(preds, targets):\n",
        "    preds = preds.sigmoid()\n",
        "    return ((preds > 0.5) == targets).float().mean()\n",
        "\n",
        "# Train model\n",
        "lr = 1.\n",
        "for epoch in range(20):\n",
        "    # Training phase\n",
        "    for xb,yb in train_dset:\n",
        "        preds = linear1(xb)\n",
        "        loss = mnist_loss(preds, yb)\n",
        "        loss.backward()\n",
        "        weights.data -= weights.grad*lr\n",
        "        bias.data -= bias.grad*lr\n",
        "        weights.grad.zero_()\n",
        "        bias.grad.zero_()\n",
        "\n",
        "# Validation phase\n",
        "valid_preds = [linear1(xb) for xb,yb in valid_dset]\n",
        "valid_loss = mnist_loss(torch.cat(valid_preds), valid_y)\n",
        "valid_acc = accuracy(torch.cat(valid_preds), valid_y)\n",
        "print(f\"Epoch {epoch}: Valid Loss: {valid_loss}, Valid Acc: {valid_acc}\")\n",
        "from fastai.vision.all import *\n",
        "from fastai.vision.core import *\n",
        "from fastai.vision.data import *\n",
        "from fastai.vision.learner import *\n",
        "from fastai.vision.models import *\n",
        "from fastai.metrics import *\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer.\n",
        "from fastai.vision.all import *\n",
        "\n",
        "# Load data\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "train_items = threes + sevens\n",
        "train_labels = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
        "train_dset = [(PILImage.create(item), label) for item, label in zip(train_items, train_labels)]\n",
        "dls = DataLoader(train_dset, batch_size=256)\n",
        "\n",
        "# Define model\n",
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(28*28, 50)\n",
        "        self.layer2 = nn.Linear(50, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = TwoLayerNet()\n",
        "opt = SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Define loss function and metric\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "metrics = accuracy_multi\n",
        "\n",
        "# Train model\n",
        "learn = Learner(dls, model, opt_func=opt, loss_func=loss_func, metrics=metrics)\n",
        "learn.fit(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}