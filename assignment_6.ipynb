{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Camel-light/Assignments/blob/main/assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following assignment consists of a theoretical part (learning portfolio) and a practical part (assignment). The goal is to build a classification model that predicts from which subject area a certain abstract originates. The plan would be that next week we will discuss your learnings from the theory part, that means you are relatively free to fill your Learning Portfolio on this new topic and in two weeks we will discuss your solutions of the Classification Model.\n",
        "\n",
        "#Theory part (filling your Learning Portfolio, May 10)\n",
        "\n",
        "In preparation for the practical part, I ask you to familiarize yourself with the following resources in the next week:\n",
        "\n",
        "1) Please watch the following video:\n",
        "\n",
        "https://course.fast.ai/Lessons/lesson4.html\n",
        "\n",
        "You are also welcome to watch the accompanying Kaggle notebook if you like the video.\n",
        "\n",
        "2) In addition to the video, I recommend you to read the first chapters of the course\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "\n",
        "\n",
        "Try to understand principle processes and log them in your learning portfolio! A few suggestions: What is a pre-trained NLP model? How do I load them? What is tokenization? What does fine-tuning mean? What types of NLP Models are there? What possibilities do I have with the Transformers package? etc...\n",
        "\n",
        "#Practical part (Assignment, May 17)\n",
        "\n",
        "1) Preprocessing: The data which I provide as zip in Olat must be processed first, that means we need a table which has the following form:\n",
        "\n",
        "Keywords | Title | Abstract | Research Field\n",
        "\n",
        "The research field is determined by the name of the file.\n",
        "\n",
        "2) We need a training dataset and a test dataset. My suggestion would be that for each research field we use the first 5700 lines for the training dataset and the last 300 lines for the test dataset. Please stick to this because then we can compare our models better!\n",
        "\n",
        "3) Please use a pre-trained model from huggingface to build a classification model that tries to predict the correct research field from the 26. Please calculate the accuracy and the overall accuracy for all research fields. If you solve this task in a group, you can also try different pre-trained models. In addition to the abstracts, you can also see if the model improves if you include keywords and titles.\n",
        "\n",
        "Some links, which can help you:\n",
        "\n",
        "https://huggingface.co/docs/transformers/training\n",
        "\n",
        "https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "One last request: Please always use PyTorch and not TensorFlow!"
      ],
      "metadata": {
        "id": "Rv37EvemaCce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUi9az-cZ9zq"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the fields unique occurrences (set)\n",
        "import os\n",
        "\n",
        "def get_fields(directory):\n",
        "    fields = set()\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.csv'):\n",
        "            field = filename.split('_')[0]\n",
        "            fields.add(field)\n",
        "    return list(fields)\n",
        "\n",
        "\n",
        "directory = '/content/drive/MyDrive/Colab_Notebooks/Lab6/data_cleaned'\n",
        "fields = get_fields(directory)\n",
        "print(fields)"
      ],
      "metadata": {
        "id": "YpmuT7uUirrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data from each csv, put first 5700rows into train_df and 300 into test_df\n",
        "import pandas as pd\n",
        "\n",
        "def get_data(directory):\n",
        "    train_dfs = []\n",
        "    test_dfs = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            df = pd.read_csv(filepath)\n",
        "            train_df = df.iloc[:5700]\n",
        "            test_df = df.iloc[-300:]\n",
        "            train_dfs.append(train_df)\n",
        "            test_dfs.append(test_df)\n",
        "    train_data = pd.concat(train_dfs)\n",
        "    test_data = pd.concat(test_dfs)\n",
        "    return train_data, test_data\n",
        "\n",
        "train_data, test_data = get_data(directory)"
      ],
      "metadata": {
        "id": "3H8WCCVmoYcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "aRd8_dKiqIzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "xlPdvHOhqopD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "O8lIHlyHqqgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.describe()"
      ],
      "metadata": {
        "id": "xXi3pyeSq6P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()\n",
        "test_data.info()"
      ],
      "metadata": {
        "id": "nzNv1LQ8r5ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(5700/6000)\n",
        "print(154594/(154594+23400))"
      ],
      "metadata": {
        "id": "K1dDeHUnrVBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like not all files contain a total of 6000 rows. One possibility would be to always take a ratio of 95% / 5% for train / test respectively. Sticking with taking the first 5700 for train as demanded in assignment."
      ],
      "metadata": {
        "id": "SgUZylUHzM6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Many 'Index Keywords' fields are empty. Imputing with row above, only if it has the same research field, as they are about similar topics.\n",
        "\n",
        "def fill_missing_keywords(df):\n",
        "    prev_keywords = None\n",
        "    prev_field = None\n",
        "    filled_keywords = []\n",
        "    for index, row in df.iterrows():\n",
        "        keywords = row['Index Keywords']\n",
        "        field = row['Research Field']\n",
        "        if pd.isnull(keywords) and field == prev_field:\n",
        "            filled_keywords.append(prev_keywords)\n",
        "        else:\n",
        "            filled_keywords.append(keywords)\n",
        "            prev_keywords = keywords\n",
        "        prev_field = field\n",
        "    return filled_keywords\n",
        "\n",
        "train_data['Index Keywords'] = fill_missing_keywords(train_data)\n",
        "test_data['Index Keywords'] = fill_missing_keywords(test_data)"
      ],
      "metadata": {
        "id": "RyECuFCOxsQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()\n",
        "test_data.info()"
      ],
      "metadata": {
        "id": "O8mgDR71yBc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now there are less null rows for Index Keywords (more non-null rows)"
      ],
      "metadata": {
        "id": "cSeTzyF9z4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=26)"
      ],
      "metadata": {
        "id": "8RRrFi6FuNON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import Dataset, ClassLabel\n",
        "\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    # Select the input features and labels\n",
        "    input_features = ['Title']\n",
        "    input_text = data[input_features].iloc[:,0]\n",
        "    labels = data['Research Field'].to_numpy()\n",
        "    \n",
        "    # Tokenize the input text\n",
        "    encodings = tokenizer(input_text.to_list(), truncation=True, padding=True)\n",
        "    \n",
        "    # Get the unique label names from your data\n",
        "    label_names = data['Research Field'].unique()\n",
        "\n",
        "    # Create a ClassLabel object with the unique label names\n",
        "    label_feature = ClassLabel(names=label_names)\n",
        "\n",
        "    # Encode the labels in your training data\n",
        "    labels = [label_feature.str2int(label) for label in labels]\n",
        "    \n",
        "    # Convert the data to a format that can be used by the Trainer\n",
        "    dataset = Dataset.from_dict({**encodings, 'labels': labels})\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Preprocess the training and test data\n",
        "train_data = preprocess_data(train_data)\n",
        "test_data = preprocess_data(test_data)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Create a Trainer and fine-tune the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=train_data.features['labels'].num_classes)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "aXgPFPwE4klj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Define a function to preprocess the data\n",
        "def preprocess(data, batch_size=1000):\n",
        "    input_features = ['Title']\n",
        "    input_text = data[input_features].iloc[:,0]\n",
        "    labels = data['Research Field']\n",
        "    \n",
        "    # Tokenize the input text in batches\n",
        "    encodings = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n",
        "    for i in range(0, len(input_text), batch_size):\n",
        "        batch = input_text[i:i+batch_size]\n",
        "        batch_encodings = tokenizer(batch.to_list(), truncation=True, padding=True)\n",
        "        encodings['input_ids'].extend(batch_encodings['input_ids'])\n",
        "        encodings['attention_mask'].extend(batch_encodings['attention_mask'])\n",
        "        if 'token_type_ids' in batch_encodings:\n",
        "            encodings['token_type_ids'].extend(batch_encodings['token_type_ids'])\n",
        "    \n",
        "    return encodings, labels\n",
        "\n",
        "# Preprocess the training and test data\n",
        "train_encodings, train_labels = preprocess(train_data)\n",
        "test_encodings, test_labels = preprocess(test_data)\n",
        "\n",
        "# Convert the labels to a NumPy array\n",
        "train_labels = train_labels.to_numpy()\n",
        "test_labels = test_labels.to_numpy()\n",
        "\n",
        "# Convert the data to a format that can be used by the Trainer\n",
        "train_data = CustomDataset(train_encodings, train_labels)\n",
        "test_data = CustomDataset(test_encodings, test_labels)\n",
        "\n",
        "# Convert the data to a format that can be used by the Trainer\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_data = CustomDataset(train_encodings, train_labels)\n",
        "test_data = CustomDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "8VLsfMIH8Ru-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of train_labels: {type(train_labels)}\")\n",
        "print(f\"Shape of train_labels: {train_labels.shape}\")\n",
        "print(f\"Type of test_labels: {type(test_labels)}\")\n",
        "print(f\"Shape of test_labels: {test_labels.shape}\")"
      ],
      "metadata": {
        "id": "hLKCfsqs2glU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataset(dataset):\n",
        "    # Check if the dataset has a 'labels' key\n",
        "    if 'labels' not in dataset[0]:\n",
        "        print(\"The dataset does not contain a 'labels' key.\")\n",
        "    else:\n",
        "        print(\"The dataset contains a 'labels' key.\")\n",
        "        # Check the shape of the labels\n",
        "        labels_shape = dataset[0]['labels'].shape\n",
        "        print(f\"The shape of the labels is: {labels_shape}\")\n",
        "\n",
        "# Analyze the training and test datasets\n",
        "print(\"Analyzing the training dataset:\")\n",
        "analyze_dataset(train_data)\n",
        "print(\"\\nAnalyzing the test dataset:\")\n",
        "analyze_dataset(test_data)"
      ],
      "metadata": {
        "id": "gBMHHiF72HA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Create a Trainer and fine-tune the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "C0efhTM0dPiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "results = trainer.evaluate()\n",
        "print(f'Overall accuracy: {results[\"eval_accuracy\"]}')"
      ],
      "metadata": {
        "id": "wvtGV-eJdUAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addition: Accuracy measures whether the research field with the highest probability value matches the target. With 26 research fields, it would also be interesting to know if the correct target is at least among the three highest probability values.\n",
        "\n",
        "$\\begin{pmatrix} A\\\\ B \\\\ C \\\\D \\\\E \\end{pmatrix} = \\begin{pmatrix} 0.1\\\\ 0.95 \\\\ 0.5 \\\\0.2 \\\\0.3 \\end{pmatrix} → \\text{Choice}_1 = B, \\text{Choice}_3 = B,C,E$"
      ],
      "metadata": {
        "id": "Up3aCw__4f4d"
      }
    }
  ]
}